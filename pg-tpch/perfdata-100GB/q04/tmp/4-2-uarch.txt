vtune: Warning: To profile kernel modules during the session, make sure they are available in the /lib/modules/kernel_version/ location.
vtune: Collection started. To stop the collection, either press CTRL-C or enter from another console window: vtune -r /scratchNVME/pg-tpch/perfdata-100GB/q04/4-2-uarch -command stop.
vtune: Collection detached.
vtune: Collection stopped.
vtune: Using result path `/scratchNVME/pg-tpch/perfdata-100GB/q04/4-2-uarch'
vtune: Executing actions  0 %                                                  vtune: Executing actions  0 % Finalizing results                               vtune: Executing actions  0 % Finalizing the result                            vtune: Executing actions  0 % Clearing the database                            vtune: Executing actions  7 % Clearing the database                            vtune: Executing actions  7 % Loading raw data to the database                 vtune: Executing actions  7 % Loading 'systemcollector-2102360-ares.sc' file   vtune: Executing actions 12 % Loading 'systemcollector-2102360-ares.sc' file   vtune: Executing actions 12 % Loading 'system-wide.perf' file                  vtune: Executing actions 12 % Updating precomputed scalar metrics              vtune: Executing actions 14 % Updating precomputed scalar metrics              vtune: Executing actions 14 % Processing profile metrics and debug information vtune: Executing actions 19 % Processing profile metrics and debug information vtune: Executing actions 19 % Setting data model parameters                    vtune: Executing actions 19 % Resolving module symbols                         vtune: Executing actions 19 % Resolving information for `[vdso]'               vtune: Executing actions 19 % Resolving information for `nvme.ko'              vtune: Executing actions 19 % Resolving information for `igb.ko'               vtune: Executing actions 19 % Resolving information for `libm.so.6'            vtune: Executing actions 19 % Resolving information for `nf_conntrack.ko'      vtune: Executing actions 19 % Resolving information for `nf_tables.ko'         
vtune: Warning: Cannot locate debugging information for file `/lib/modules/5.15.0-47-generic/kernel/drivers/net/ethernet/intel/igb/igb.ko'.
vtune: Warning: Cannot read load addresses of sections from `/sys/module/igb/sections'. This may affect the correctness of symbol resolution for `/lib/modules/5.15.0-47-generic/kernel/drivers/net/ethernet/intel/igb/igb.ko'. Make sure this directory exists and all files in this directory have read permissions.
vtune: Executing actions 19 % Resolving information for `libc.so.6'            
vtune: Warning: Cannot locate debugging information for file `/lib/modules/5.15.0-47-generic/kernel/net/netfilter/nf_tables.ko'.
vtune: Warning: Cannot locate debugging information for file `/lib/modules/5.15.0-47-generic/kernel/drivers/nvme/host/nvme.ko'.
vtune: Warning: Cannot read load addresses of sections from `/sys/module/nf_tables/sections'. This may affect the correctness of symbol resolution for `/lib/modules/5.15.0-47-generic/kernel/net/netfilter/nf_tables.ko'. Make sure this directory exists and all files in this directory have read permissions.
vtune: Warning: Cannot read load addresses of sections from `/sys/module/nvme/sections'. This may affect the correctness of symbol resolution for `/lib/modules/5.15.0-47-generic/kernel/drivers/nvme/host/nvme.ko'. Make sure this directory exists and all files in this directory have read permissions.
vtune: Warning: Cannot locate debugging information for file `/lib/modules/5.15.0-47-generic/kernel/net/netfilter/nf_conntrack.ko'.
vtune: Warning: Cannot read load addresses of sections from `/sys/module/nf_conntrack/sections'. This may affect the correctness of symbol resolution for `/lib/modules/5.15.0-47-generic/kernel/net/netfilter/nf_conntrack.ko'. Make sure this directory exists and all files in this directory have read permissions.
vtune: Executing actions 19 % Resolving information for `nvme-core.ko'         
vtune: Warning: Cannot locate debugging information for file `/lib/modules/5.15.0-47-generic/kernel/drivers/nvme/host/nvme-core.ko'.
vtune: Warning: Cannot read load addresses of sections from `/sys/module/nvme-core/sections'. This may affect the correctness of symbol resolution for `/lib/modules/5.15.0-47-generic/kernel/drivers/nvme/host/nvme-core.ko'. Make sure this directory exists and all files in this directory have read permissions.
vtune: Executing actions 20 % Resolving information for `nvme-core.ko'         vtune: Executing actions 21 % Resolving information for `nvme-core.ko'         vtune: Executing actions 21 % Resolving information for `postgres'             
vtune: Warning: Cannot locate file `vmlinux'.
vtune: Executing actions 21 % Resolving information for `vmlinux'              
vtune: Warning: Cannot locate debugging information for the Linux kernel. Source-level analysis will not be possible. Function-level analysis will be limited to kernel symbol tables. See the Enabling Linux Kernel Analysis topic in the product online help for instructions.
vtune: Executing actions 22 % Resolving information for `vmlinux'              vtune: Executing actions 22 % Resolving bottom user stack information          vtune: Executing actions 23 % Resolving bottom user stack information          vtune: Executing actions 23 % Resolving thread name information                vtune: Executing actions 24 % Resolving thread name information                vtune: Executing actions 24 % Resolving call target names for dynamic code     vtune: Executing actions 25 % Resolving call target names for dynamic code     vtune: Executing actions 25 % Resolving interrupt name information             vtune: Executing actions 26 % Resolving interrupt name information             vtune: Executing actions 26 % Processing profile metrics and debug information vtune: Executing actions 27 % Processing profile metrics and debug information vtune: Executing actions 28 % Processing profile metrics and debug information vtune: Executing actions 29 % Processing profile metrics and debug information vtune: Executing actions 30 % Processing profile metrics and debug information vtune: Executing actions 30 % Setting data model parameters                    vtune: Executing actions 30 % Precomputing frequently used data                vtune: Executing actions 30 % Precomputing frequently used data                vtune: Executing actions 31 % Precomputing frequently used data                vtune: Executing actions 32 % Precomputing frequently used data                vtune: Executing actions 33 % Precomputing frequently used data                vtune: Executing actions 34 % Precomputing frequently used data                vtune: Executing actions 35 % Precomputing frequently used data                vtune: Executing actions 36 % Precomputing frequently used data                vtune: Executing actions 36 % Updating precomputed scalar metrics              vtune: Executing actions 37 % Updating precomputed scalar metrics              vtune: Executing actions 37 % Discarding redundant overtime data               vtune: Executing actions 39 % Discarding redundant overtime data               vtune: Executing actions 39 % Saving the result                                vtune: Executing actions 41 % Saving the result                                vtune: Executing actions 42 % Saving the result                                vtune: Executing actions 50 % Saving the result                                vtune: Executing actions 50 % Generating a report                              vtune: Executing actions 50 % Setting data model parameters                    vtune: Executing actions 75 % Setting data model parameters                    vtune: Executing actions 75 % Generating a report                              Elapsed Time: 270.427s
    Clockticks: 947,664,000,000
    Instructions Retired: 801,636,000,000
    CPI Rate: 1.182
     | The CPI may be too high. This could be caused by issues such as memory
     | stalls, instruction starvation, branch misprediction or long latency
     | instructions. Explore the other hardware-related metrics to identify what
     | is causing high CPI.
     |
    Retiring: 23.4% of Pipeline Slots
        Light Operations: 20.6% of Pipeline Slots
        Heavy Operations: 2.8% of Pipeline Slots
            Microcode Sequencer: 2.8% of Pipeline Slots
                Assists: 0.0% of Pipeline Slots
                CISC: 2.8% of Pipeline Slots
    Front-End Bound: 19.3% of Pipeline Slots
     | Issue: A significant portion of Pipeline Slots is remaining empty due to
     | issues in the Front-End.
     | 
     | Tips:  Make sure the code working size is not too large, the code layout
     | does not require too many memory accesses per cycle to get enough
     | instructions for filling four pipeline slots, or check for microcode
     | assists.
     |
        Front-End Latency: 11.2% of Pipeline Slots
         | This metric represents a fraction of slots during which CPU was
         | stalled due to front-end latency issues, such as instruction-cache
         | misses, ITLB misses or fetch stalls after a branch misprediction. In
         | such cases, the front-end delivers no uOps.
         |
            ICache Misses: 7.2% of Clockticks
             | Issue: A significant portion of instruction fetches is missing in
             | the instruction cache.
             | 
             | Tips:
             | 
             | 1. Use profile-guided optimization to reduce the size of hot code
             | regions.
             | 
             | 2. Consider compiler options to reorder functions so that hot
             | functions are located together.
             | 
             | 3. If your application makes significant use of macros, try to
             | reduce this by either converting the relevant macros to functions
             | or using linker options to eliminate repeated code.
             | 
             | 4. Consider the Os/O1 optimization level or the following subset
             | of optimizations to decrease your code footprint:
             |     - use inlining only when it decreases the footprint
             |     - disable loop unrolling
             |     - disable intrinsic inlining
             | 
             | Optimization examples:
             | Instruction Cache Misses recipe from Intel VTune Profiler
             | Performance Analysis Cookbook
             |
            ITLB Overhead: 1.0% of Clockticks
            Branch Resteers: 6.3% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to Branch
             | Resteers. Branch Resteers estimate the Front-End delay in
             | fetching operations from corrected path, following all sorts of
             | mispredicted branches. For example, branchy code with lots of
             | mispredictions might get categorized as Branch Resteers. Note the
             | value of this node may overlap its siblings.
             |
            DSB Switches: 1.3% of Clockticks
            Length Changing Prefixes: 0.3% of Clockticks
            MS Switches: 2.8% of Clockticks
        Front-End Bandwidth: 8.1% of Pipeline Slots
            Front-End Bandwidth MITE: 9.5% of Pipeline Slots
            Front-End Bandwidth DSB: 1.9% of Pipeline Slots
            (Info) DSB Coverage: 38.0%
    Bad Speculation: 3.8% of Pipeline Slots
        Branch Mispredict: 3.7% of Pipeline Slots
        Machine Clears: 0.1% of Pipeline Slots
    Back-End Bound: 53.6% of Pipeline Slots
     | A significant portion of pipeline slots are remaining empty. When
     | operations take too long in the back-end, they introduce bubbles in the
     | pipeline that ultimately cause fewer pipeline slots containing useful
     | work to be retired per cycle than the machine is capable to support. This
     | opportunity cost results in slower execution. Long-latency operations
     | like divides and memory operations can cause this, as can too many
     | operations being directed to a single execution port (for example, more
     | multiply operations arriving in the back-end per cycle than the execution
     | unit can support).
     |
        Memory Bound: 42.9% of Pipeline Slots
         | The metric value is high. This can indicate that the significant
         | fraction of execution pipeline slots could be stalled due to demand
         | memory load and stores. Use Memory Access analysis to have the metric
         | breakdown by memory hierarchy, memory bandwidth information,
         | correlation by memory objects.
         |
            L1 Bound: 13.6% of Clockticks
             | This metric shows how often machine was stalled without missing
             | the L1 data cache. The L1 cache typically has the shortest
             | latency. However, in certain cases like loads blocked on older
             | stores, a load might suffer a high latency even though it is
             | being satisfied by the L1. Note that this metric value may be
             | highlighted due to DTLB Overhead or Cycles of 1 Port Utilized
             | issues.
             |
                DTLB Overhead: 9.1% of Clockticks
                Loads Blocked by Store Forwarding: 1.6% of Clockticks
                Lock Latency: 0.4% of Clockticks
                 | A significant fraction of CPU cycles spent handling cache
                 | misses due to lock operations. Due to the microarchitecture
                 | handling of locks, they are classified as L1 Bound regardless
                 | of what memory source satisfied them. Note that this metric
                 | value may be highlighted due to Store Latency issue.
                 |
                Split Loads: 0.0% of Clockticks
                4K Aliasing: 0.3% of Clockticks
                FB Full: 100.0% of Clockticks
                 | This metric does a rough estimation of how often L1D Fill
                 | Buffer unavailability limited additional L1D miss memory
                 | access requests to proceed. The higher the metric value, the
                 | deeper the memory hierarchy level the misses are satisfied
                 | from. Often it hints on approaching bandwidth limits (to L2
                 | cache, L3 cache or external memory). Avoid adding software
                 | prefetches if indeed memory BW limited.
                 |
            L2 Bound: 1.4% of Clockticks
            L3 Bound: 6.2% of Clockticks
             | This metric shows how often CPU was stalled on L3 cache, or
             | contended with a sibling Core. Avoiding cache misses (L2
             | misses/L3 hits) improves the latency and increases performance.
             |
                Contested Accesses: 0.3% of Clockticks
                 | Issues: There is a high number of contested accesses to
                 | cachelines modified by another core.
                 | 
                 | Tips: Consider either using techniques suggested for other
                 | long latency load events (for example, LLC Miss) or reducing
                 | the contested accesses. To reduce contested accesses, first
                 | identify the cause. If it is a synchronization, try
                 | increasing synchronization granularity. If it is true data
                 | sharing, consider data privatization and reduction. If it is
                 | false data sharing, restructure the data to place contested
                 | variables into distinct cachelines. This may increase the
                 | working set due to padding, but false sharing can always be
                 | avoided.
                 |
                Data Sharing: 0.0% of Clockticks
                 | Issue: Significant data sharing by different cores is
                 | detected.
                 | 
                 | Tips:
                 | 
                 | 1. Examine the Contested Accesses metric to determine whether
                 | the major component of data sharing is due to contested
                 | accesses or simple read sharing. Read sharing is a lower
                 | priority than Contested Accesses or issues such as LLC Misses
                 | and Remote Accesses.
                 | 
                 | 2. If simple read sharing is a performance bottleneck,
                 | consider changing data layout across threads or rearranging
                 | computation. However, this type of tuning may not be
                 | straightforward and could bring more serious performance
                 | issues back.
                 |
                L3 Latency: 6.5% of Clockticks
                 | This metric shows a fraction of cycles with demand load
                 | accesses that hit the L3 cache under unloaded scenarios
                 | (possibly L3 latency limited). Avoiding private cache misses
                 | (i.e. L2 misses/L3 hits) will improve the latency, reduce
                 | contention with sibling physical cores and increase
                 | performance. Note the value of this node may overlap with its
                 | siblings.
                 |
                SQ Full: 0.3% of Clockticks
                 | This metric measures fraction of cycles where the Super Queue
                 | (SQ) was full taking into account all request-types and both
                 | hardware SMT threads. The Super Queue is used for requests to
                 | access the L2 cache or to go out to the Uncore.
                 |
            DRAM Bound: 18.6% of Clockticks
             | This metric shows how often CPU was stalled on the main memory
             | (DRAM). Caching typically improves the latency and increases
             | performance.
             |
                Memory Bandwidth: 2.1% of Clockticks
                 | Issue: A significant fraction of cycles was stalled due to
                 | approaching bandwidth limits of the main memory (DRAM).
                 | 
                 | Tips: Improve data accesses to reduce cacheline transfers
                 | from/to memory using these possible techniques:
                 |     - Consume all bytes of each cacheline before it is
                 |       evicted (for example, reorder structure elements and
                 |       split non-hot ones).
                 |     - Merge compute-limited and bandwidth-limited loops.
                 |     - Use NUMA optimizations on a multi-socket system.
                 | 
                 | Note: software prefetches do not help a bandwidth-limited
                 | application.
                 |
                Memory Latency: 40.2% of Clockticks
                 | Issue: A significant fraction of cycles was stalled due to
                 | the latency of the main memory (DRAM).
                 | 
                 | Tips: Improve data accesses or interleave them with compute
                 | using such possible techniques as data layout re-structuring
                 | or software prefetches (through the compiler).
                 |
                    Local DRAM: 5.9% of Clockticks
                    Remote DRAM: 0.0% of Clockticks
                    Remote Cache: 5.4% of Clockticks
                     | The number of CPU stalls on loads from the remote cache
                     | exceeds the threshold. This is often caused by non-
                     | optimal NUMA memory allocations.
                     |
            Store Bound: 14.8% of Clockticks
                Store Latency: 31.4% of Clockticks
                False Sharing: 0.0% of Clockticks
                Split Stores: 0.0% of Clockticks
                DTLB Store Overhead: 2.3% of Clockticks
        Core Bound: 10.6% of Pipeline Slots
         | This metric represents how much Core non-memory issues were of a
         | bottleneck. Shortage in hardware compute resources, or dependencies
         | software's instructions are both categorized under Core Bound. Hence
         | it may indicate the machine ran out of an OOO resources, certain
         | execution units are overloaded or dependencies in program's data- or
         | instruction- flow are limiting the performance (e.g. FP-chained long-
         | latency arithmetic operations).
         |
            Divider: 0.0% of Clockticks
            Port Utilization: 13.5% of Clockticks
                Cycles of 0 Ports Utilized: 41.2% of Clockticks
                Cycles of 1 Port Utilized: 12.0% of Clockticks
                Cycles of 2 Ports Utilized: 10.7% of Clockticks
                Cycles of 3+ Ports Utilized: 19.8% of Clockticks
                    ALU Operation Utilization: 14.0% of Clockticks
                        Port 0: 10.7% of Clockticks
                        Port 1: 11.3% of Clockticks
                        Port 5: 11.9% of Clockticks
                        Port 6: 22.1% of Clockticks
                    Load Operation Utilization: 13.0% of Clockticks
                        Port 2: 17.2% of Clockticks
                        Port 3: 17.5% of Clockticks
                    Store Operation Utilization: 17.9% of Clockticks
                        Port 4: 17.9% of Clockticks
                        Port 7: 9.3% of Clockticks
    Average CPU Frequency: 3.015 GHz
    Total Thread Count: 12
    Paused Time: 0s
Effective CPU Utilization: 7.3%
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization caused by load imbalance, threading runtime overhead, contended
 | synchronization, or thread/process underutilization. Explore sub-metrics to
 | estimate the efficiency of MPI and OpenMP parallelism or run the Locks and
 | Waits analysis to identify parallel bottlenecks for other parallel runtimes.
 |
    Average Effective CPU Utilization: 1.160 out of 16
Collection and Platform Info
    Operating System: 5.15.0-47-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=22.04 DISTRIB_CODENAME=jammy DISTRIB_DESCRIPTION="Ubuntu 22.04.1 LTS"
    Computer Name: ares
    Result Size: 77.3 MB 
    Collection start time: 15:20:15 18/09/2022 UTC
    Collection stop time: 15:24:45 18/09/2022 UTC
    Collector Type: Driverless Perf system-wide sampling
    CPU
        Name: Intel(R) Xeon(R) E5/E7 v3 Processor code named Haswell
        Frequency: 2.400 GHz
        Logical CPU Count: 16
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: not detected

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
vtune: Executing actions 100 % Generating a report                             vtune: Executing actions 100 % done                                            
