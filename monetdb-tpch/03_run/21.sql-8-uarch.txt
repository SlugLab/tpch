vtune: Using result path `/scratchNVME/tpch/monetdb-tpch/03_run/21.sql-8-uarch'
vtune: Executing actions 75 % Generating a report                              Elapsed Time: 3594.938s
    Clockticks: 211,488,000,000
    Instructions Retired: 311,448,000,000
    CPI Rate: 0.679
    Retiring: 43.5% of Pipeline Slots
        Light Operations: 40.8% of Pipeline Slots
        Heavy Operations: 2.7% of Pipeline Slots
            Microcode Sequencer: 2.7% of Pipeline Slots
                Assists: 0.0% of Pipeline Slots
                CISC: 2.7% of Pipeline Slots
    Front-End Bound: 11.0% of Pipeline Slots
        Front-End Latency: 4.8% of Pipeline Slots
            ICache Misses: 0.1% of Clockticks
            ITLB Overhead: 0.3% of Clockticks
            Branch Resteers: 2.0% of Clockticks
            DSB Switches: 0.0% of Clockticks
            Length Changing Prefixes: 0.0% of Clockticks
            MS Switches: 0.0% of Clockticks
        Front-End Bandwidth: 6.1% of Pipeline Slots
            Front-End Bandwidth MITE: 6.8% of Pipeline Slots
            Front-End Bandwidth DSB: 7.3% of Pipeline Slots
            (Info) DSB Coverage: 55.0%
    Bad Speculation: 10.8% of Pipeline Slots
        Branch Mispredict: 0.0% of Pipeline Slots
        Machine Clears: 10.8% of Pipeline Slots
    Back-End Bound: 34.7% of Pipeline Slots
     | A significant portion of pipeline slots are remaining empty. When
     | operations take too long in the back-end, they introduce bubbles in the
     | pipeline that ultimately cause fewer pipeline slots containing useful
     | work to be retired per cycle than the machine is capable to support. This
     | opportunity cost results in slower execution. Long-latency operations
     | like divides and memory operations can cause this, as can too many
     | operations being directed to a single execution port (for example, more
     | multiply operations arriving in the back-end per cycle than the execution
     | unit can support).
     |
        Memory Bound: 17.6% of Pipeline Slots
            L1 Bound: 10.7% of Clockticks
                DTLB Overhead: 0.0% of Clockticks
                Loads Blocked by Store Forwarding: 1.8% of Clockticks
                Lock Latency: 0.0% of Clockticks
                Split Loads: 0.0% of Clockticks
                4K Aliasing: 0.1% of Clockticks
                FB Full: 0.0% of Clockticks
            L2 Bound: 0.0% of Clockticks
            L3 Bound: 0.0% of Clockticks
                Contested Accesses: 0.0% of Clockticks
                Data Sharing: 0.0% of Clockticks
                L3 Latency: 0.0% of Clockticks
                SQ Full: 0.0% of Clockticks
            DRAM Bound: 8.7% of Clockticks
                Memory Bandwidth: 0.0% of Clockticks
                Memory Latency: 39.0% of Clockticks
                    Local DRAM: 0.0% of Clockticks
                    Remote DRAM: 0.0% of Clockticks
                    Remote Cache: 0.0% of Clockticks
            Store Bound: 7.6% of Clockticks
                Store Latency: 38.4% of Clockticks
                False Sharing: 0.0% of Clockticks
                Split Stores: 0.0% of Clockticks
                DTLB Store Overhead: 0.3% of Clockticks
        Core Bound: 17.2% of Pipeline Slots
         | This metric represents how much Core non-memory issues were of a
         | bottleneck. Shortage in hardware compute resources, or dependencies
         | software's instructions are both categorized under Core Bound. Hence
         | it may indicate the machine ran out of an OOO resources, certain
         | execution units are overloaded or dependencies in program's data- or
         | instruction- flow are limiting the performance (e.g. FP-chained long-
         | latency arithmetic operations).
         |
            Divider: 0.0% of Clockticks
            Port Utilization: 23.2% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to Core
             | non-divider-related issues.
             | 
             | Tips: Use vectorization to reduce pressure on the execution ports
             | as multiple elements are calculated with same uOp.
             |
                Cycles of 0 Ports Utilized: 25.1% of Clockticks
                 | CPU executed no uOps on any execution port during a
                 | significant fraction of cycles. Long-latency instructions
                 | like divides may contribute to this issue. Check the Assembly
                 | view and Appendix C in the Optimization Guide to identify
                 | instructions with 5 or more cycles latency.
                 |
                Cycles of 1 Port Utilized: 14.2% of Clockticks
                 | This metric represents cycles fraction where the CPU executed
                 | total of 1 uop per cycle on all execution ports. This can be
                 | due to heavy data-dependency among software instructions, or
                 | oversubscribing a particular hardware resource. In some other
                 | cases with high 1_Port_Utilized and L1 Bound, this metric can
                 | point to L1 data-cache latency bottleneck that may not
                 | necessarily manifest with complete execution starvation (due
                 | to the short L1 latency e.g. walking a linked list) - looking
                 | at the assembly can be helpful. Note that this metric value
                 | may be highlighted due to L1 Bound issue.
                 |
                Cycles of 2 Ports Utilized: 12.7% of Clockticks
                Cycles of 3+ Ports Utilized: 47.7% of Clockticks
                    ALU Operation Utilization: 28.5% of Clockticks
                        Port 0: 21.0% of Clockticks
                        Port 1: 25.4% of Clockticks
                        Port 5: 22.7% of Clockticks
                        Port 6: 44.9% of Clockticks
                    Load Operation Utilization: 43.7% of Clockticks
                        Port 2: 49.3% of Clockticks
                        Port 3: 45.0% of Clockticks
                    Store Operation Utilization: 27.6% of Clockticks
                        Port 4: 27.6% of Clockticks
                        Port 7: 20.8% of Clockticks
    Average CPU Frequency
    Total Thread Count: 21
    Paused Time: 0s
Effective CPU Utilization: 0.0%
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization caused by load imbalance, threading runtime overhead, contended
 | synchronization, or thread/process underutilization. Explore sub-metrics to
 | estimate the efficiency of MPI and OpenMP parallelism or run the Locks and
 | Waits analysis to identify parallel bottlenecks for other parallel runtimes.
 |
    Average Effective CPU Utilization: 0.000 out of 16
Collection and Platform Info
    Operating System: 5.15.0-47-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=22.04 DISTRIB_CODENAME=jammy DISTRIB_DESCRIPTION="Ubuntu 22.04.1 LTS"
    Computer Name: ares
    Result Size: 350.6 MB 
    Collection start time: 13:41:13 16/09/2022 UTC
    Collection stop time: 14:41:08 16/09/2022 UTC
    Collector Type: Driverless Perf system-wide sampling
    CPU
        Name: Intel(R) Xeon(R) E5/E7 v3 Processor code named Haswell
        Frequency: 2.400 GHz
        Logical CPU Count: 16
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: not detected

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
vtune: Executing actions 100 % done  